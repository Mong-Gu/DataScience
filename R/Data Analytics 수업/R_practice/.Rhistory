plot(fit0701_1)
groups <-cutree(fit0701_1, k = 3)
rect.hclust(fit0701_1, k = 3, border = "red")
install.packages("psych")
install.packages("psych")
library(psych)
describeBy(mydata07, groups)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 3)
library(cluster)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
install.packages("fpc")
install.packages("fpc")
library(fpc)
plotcluster(mydata07, fit0702_2$cluster)
fit0702_2 <-factanal(df07, 5, rotation = "varimax")
print(fit0702_2, digits = 2, cutoff = 0.3, sort = TRUE)
load <-fit0702_2$loadings[,1:2]
plot(load, type = "n")
text(load, labels = names(df07), cex = .7)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 3)
library(cluster)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 4)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 5)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 6)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 7)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 3)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 4)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 3)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 4)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 4)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 4)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 4)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 4)
##### 비계층적 군집분석 #####
fit0702_2 <-kmeans(mydata07, 4)
library(cluster)
clusplot(mydata07, fit0702_2$cluster, color = TRUE, xhade = TRUE, labels = 2, lines = 0)
plotcluster(mydata07, fit0702_2$cluster)
install.packages("fpc")
install.packages("fpc")
plotcluster(mydata07, fit0702_2$cluster)
fit0702_2 <-factanal(df07, 5, rotation = "varimax")
print(fit0702_2, digits = 2, cutoff = 0.3, sort = TRUE)
load <-fit0702_2$loadings[,1:2]
plot(load, type = "n")
text(load, labels = names(df07), cex = .7)
text(load, labels = names(df07), cex = .7)
plot(load, type = "n")
text(load, labels = names(df07), cex = .7)
plotcluster(mydata07, fit0702_2$cluster)
plotcluster(mydata07, fit0702_2$cluster)
plotcluster(mydata07, fit0702_2$cluster)
plot(fit0701_1)
groups <-cutree(fit0701_1, k = 3)
rect.hclust(fit0701_1, k = 3, border = "red")
describeBy(mydata07, groups)
# 1
# H0:
# H1:
# 조건 :
df <- read.csv("C:/DA2019/191107_CH06/DA예제-0601.csv")
df
(
fit <- aov(satis~com, data=df)
anova(fit)
fit <- aov(satis~com, data=df)
anova(fit)
boxplot(satis~com, col="sky blue", data = DA0601)
boxplot(satis~com, col="sky blue", data = df)
with(df, tapply(satis, com, mean))
boxplot(satis~com, col="sky blue", data = df)
TukeyHSD(fit)
# 2 (과제)
# H0:
# H1:
# 조건 :
df <- read.csv("C:/DA2019/exercise2/DA_AS02-02.csv")
df
fit <- aov(sales~program, data=df)
anova(fit)
TukeyHSD(fit)
df <-read.csv("C:/DA2019/191107_CH06//DA예제-0602.csv")
df
fit <- aov(yield ~ machine + worker, data = df)
anova(fit)
df <-read.csv("C:/DA2019/191107_CH06//DA예제-0603.csv")
df
df <-read.csv("C:/DA2019/191107_CH06//DA예제-0603.csv")
df
fit <- aov(yield ~ tem + pre + tem*pre, data = df)
anova(fit)
TukeyHSD(fit)
library(plyr)
anova <-ddply(df, c("pre", "tem"), summarise, nyield = mean(yield))
library(ggplot2)
ggplot(anova, aes(x = pre, y = nyield, color = tem, group = tem)) + geom_line()
# < 다변량분산분석 >
df <-read.csv("C:/DA2019/191107_CH06/DA예제-0606.csv")
# < 다변량분산분석 >
df <-read.csv("C:/DA2019/191107_CH06/DA예제-0606.csv")
df
fit <-manova(cbind(y1,y2) ~ x1*x2, data = df)
summary(fit, test = "Pillai")
summary(fit, test = "Pillai")
df
fit <-manova(cbind(y1,y2) ~ x1*x2, data = df)
summary(fit, test = "Pillai")
summary(fit, test = "Pillai")
# 1
# H0: μ(com1) = μ(com2) = μ(com3) = μ(com4) (즉, 각 회사 고객의 만족도 평균은 같다) (기각)
# H1: 적어도 하나 이상의 평균은 나머지와 같지 않다 (채택)
df <- read.csv("C:/DA2019/191107_CH07/DA예제-0701.csv")
# 1
# H0: μ(com1) = μ(com2) = μ(com3) = μ(com4) (즉, 각 회사 고객의 만족도 평균은 같다) (기각)
# H1: 적어도 하나 이상의 평균은 나머지와 같지 않다 (채택)
df <- read.csv("C:/DA2019/191107_CH07/DA예제-0701.csv")
# 1
# H0: μ(com1) = μ(com2) = μ(com3) = μ(com4) (즉, 각 회사 고객의 만족도 평균은 같다) (기각)
# H1: 적어도 하나 이상의 평균은 나머지와 같지 않다 (채택)
df <- read.csv("C:/DA2019/191107_CH07,C08/DA예제-0701.csv")
# 1
# H0: μ(com1) = μ(com2) = μ(com3) = μ(com4) (즉, 각 회사 고객의 만족도 평균은 같다) (기각)
# H1: 적어도 하나 이상의 평균은 나머지와 같지 않다 (채택)
df <- read.csv("C:/DA2019/191107_CH07,08/DA예제-0701.csv")
# 1
# H0: μ(com1) = μ(com2) = μ(com3) = μ(com4) (즉, 각 회사 고객의 만족도 평균은 같다) (기각)
# H1: 적어도 하나 이상의 평균은 나머지와 같지 않다 (채택)
df <- read.csv("C:/DA2019/191114_CH07/DA예제-0701.csv")
# 1
# H0: μ(com1) = μ(com2) = μ(com3) = μ(com4) (즉, 각 회사 고객의 만족도 평균은 같다) (기각)
# H1: 적어도 하나 이상의 평균은 나머지와 같지 않다 (채택)
df <- read.csv("C:/DA2019/191114_CH07,08/DA예제-0701.csv")
df
cov(df)
cov(df, use = "complete.obs")
a <- c(1,2,3,4,5)
b <- c(4,5,6,7,8)
cov(a, b)
cov(df$x, df$y)
cor.test(df)
cor.test(df, method = "pearson")
cor.test(df$x, df$y, method = "pearson")
cor.test(df$x, df$z, method = "pearson")
cor.test(df$y, df$z, method = "pearson")
cor.test(df$x, df$y, method = "pearson")
cor.test(df$x, df$z, method = "pearson")
##### DACH 08 회귀분석 #####
df <- read.csv("C:/DA2019/191114_CH07,08/DA예제-0801.csv")
plot(y ~ x, data = df)
fit <- lm(y ~ x, data = df)
summary(fit)
abline(fit)
abline(fit, col = "red")
df
# (과제)
df <- read.csv("C:/DA2019/exercise2/DA_AS02-04.csv")
df
fit <- lm(y ~ x1 + x2 + x3, data = df)
summary(fit)
library(car)
vif(car)
vif(fit)
summary(fit)
plot(fit)
# (과제)
df <- read.csv("C:/DA2019/exercise2/DA_AS02-05.csv")
df
shapiro.test(df$x1)
shapiro.test(df$x2)
shapiro.test(df$x3)
shapiro.test(df$x4)
fit_full <- glm(y ~ x1 + x2 + x3 + x4, family = binomial(), data = df)
summary(fit_full)
vif(fit_full)
fit_reduced <- glm(y ~ x4, family = binomial(), data = df)
summary(fit_reduced)
anova(fit_full, fit_reduced, test = "Chisq")
anova(fit_full, fit_reduced)
anova(fit_full, fit_reduced, test = "Chisq")
testdata <-read.csv("C:/DA2019/exercise2/DA_AS02-05test.csv")
testdata$prob <- predict(fit_reduced, newdata = testdata, type = "response")
testdata
deviance(fit_reduced)/df.residual(fit_reduced)
fit_reduced2 = step(fit_full)
summary(fit_reduced2)
##### DACH 10 판별분석 #####
df <- read.csv("C:/DA2019/191121_CH09,10/DA예제-1001.csv")
df
library(car)
scatterplotMatrix(df[1:6])
scatterplotMatrix(df[1:6])
vif_test <- lm(y~x1+x2+x3+x4, data = df)
vif_test <- lm(y~x1+x2+x3+x4, data = df)
df <- read.csv("C:/DA2019/exercise2/DA_AS02-05.csv")
df
fit_full <- glm(y ~ x1 + x2 + x3 + x4, family = binomial(), data = df)
vif(fit_full)
vif(df$x1, df$x2, df$x3, df$x4)
vif(df$x1, df$x2, df$x3, df$x4)
install.packages("MASS")
libary(MASS)
library(MASS)
fit <- lda(x1 ~ ., data = df)
fit
fit
df <- read.csv("C:/DA2019/191121_CH09,10/DA예제-1001.csv")
df
fit <- lda(x1 ~ ., data = df)
fit
fit.values <- predict(fit)
ldahist(data = fit.values$x[,1], g = x1)
ldahist(data = fit.values$x[,1], g = x1)
fit <- lda(x1 ~ ., data = df)
fit
fit.values <- predict(fit)
ldahist(data = fit.values$x[,1], g = x1)
ldahist(data = fit.values$x[,1], g = x1)
ldahist(data = fit.values$x[,1], g = x1)
df <- read.csv("C:/DA2019/exercise2/DA_AS02-05.csv")
df
shapiro.test(df$x2)
shapiro.test(df$x3)
shapiro.test(df$x4)
vif_test <- lm(y~x1+x2+x3+x4, data=df)
vif(vif_test)
fit <- lda(y ~ x1+x2+x3+x4, data = df)
fit
Mda.lda <- lda(y ~ x1+x2+x3+x4, data = df)
Mda.lda
Mda.lda.values <- predict(Mda.lda)
df
ldahist(data = Mda.lda.values$x[,1], g = y2)
ldahist(data = Mda.lda.values$x[,1], g = df$y2)
ldahist(data = Mda.lda.values$x[,1], g = df$y)
layout(matrix(c(1), 1, 1))
plot(Mda.lda.values$x[,1])
text(Mda.lda.values$x[,1], cex = 0.7, pos = 4, col = "red")
testdata <-read.csv("C:/DA2019/exercise2/DA_AS02-05test.csv")
testdata
predict(Mda.lda, newdata = testdata)
vif_test <- lm(y~x1+x2+x3+x4, data=df)
df
df$y[df$y == 1] <- 2
df$y[df$y == 0] <- 1
scatterplotMatrix(df[1:6])
scatterplotMatrix(df[1:6])
df <- read.csv("C:/DA2019/exercise2/DA_AS02-05.csv")
df
scatterplotMatrix(df[1:4])
# 1. 독립변수의 정규분포 확인
shapiro.test(df$x1)
shapiro.test(df$x2)
shapiro.test(df$x3)
shapiro.test(df$x4)
# 2. 독립변수들 간 다중공선성 확인
vif_test <- lm(y~x1+x2+x3+x4, data=df)
vif(vif_test)
Mda.lda <- lda(y ~ x1+x2+x3+x4, data = df)
Mda.lda
# 5. 누적 히스토그램 그리기
Mda.lda.values <- predict(Mda.lda)
ldahist(data = Mda.lda.values$x[,1], g = df$y)
# 6. testdata로 예측하기
testdata <-read.csv("C:/DA2019/exercise2/DA_AS02-05test.csv")
testdata
predict(Mda.lda, newdata = testdata)
df <- read.csv("C:/DA2019/exercise2/DA_AS02-05.csv")
df$y[df$y == 1] <- 2
df$y[df$y == 0] <- 1
df
scatterplotMatrix(df[1:4])
# 1. 독립변수의 정규분포 확인
shapiro.test(df$x1)
shapiro.test(df$x2)
shapiro.test(df$x3)
shapiro.test(df$x4)
# 2. 독립변수들 간 다중공선성 확인
vif_test <- lm(y~x1+x2+x3+x4, data=df)
vif(vif_test)
Mda.lda <- lda(y ~ x1+x2+x3+x4, data = df)
Mda.lda
# 5. 누적 히스토그램 그리기
Mda.lda.values <- predict(Mda.lda)
ldahist(data = Mda.lda.values$x[,1], g = df$y)
# 6. testdata로 예측하기
testdata <-read.csv("C:/DA2019/exercise2/DA_AS02-05test.csv")
predict(Mda.lda, newdata = testdata)
# < 주성분 분석 >
df <- read.csv("C:/DA2019/exercise2/DA_AS02-06.csv")
head(df)
mydata <-na.omit(df)
head(mydata)
cor(mydata)
# 3. 요인 수 결정
fit <-princomp(mydata, cor = TRUE)
summary(fit) # 4개(누적분산 64.69%) - 6개(누적분산 77.30%) 가 적당.
biplot(fit)
e_value = eigen(cor(df06))
e_value = eigen(cor(df))
e_value
plot(fit, type = "lines")
loadings(fit)
# 4. 요인부하량(각 변수와 요인간의 상관관계) 산출
loadings(fit)
fit$scores
# 7. 신뢰성 검토
FA1 <-data.frame(df$x3, df$x7, df$x8, df$x9, df$x12, df$x13)
alpha(FA1, na.rm = TRUE)
result <-PCA(mydata)
library(FactoMineR)
result <-PCA(mydata)
# 7. 신뢰성 검토
library(psych)
FA1 <-data.frame(df$x3, df$x7, df$x8, df$x9, df$x12, df$x13)
alpha(FA1, na.rm = TRUE)
FA2 <-data.frame(df$x5, df$x6)
alpha(FA2, na.rm = TRUE)
FA3 <-data.frame(df$x10, df$x11)
alpha(FA3, na.rm = TRUE)
FA4 <-data.frame(df$x2, df$x4)
alpha(FA4, na.rm = TRUE)
# < 공통요인분석 >
fit <- factanal(df, 4, rotation = "varimax")
print(fit, digits = 2, cutoff = .3, sort = TRUE)
# 3. 요인 수 결정, 4. 요인부하량 산출, 5. 요인회전 방식 결정
fit <- factanal(df, 4, rotation = "varimax")
print(fit, digits = 2, cutoff = .3, sort = TRUE)
fit <- factanal(df, 5, rotation = "varimax")
print(fit, digits = 2, cutoff = .3, sort = TRUE)
# < 계층적 군집분석 >
df <- read.csv("C:/DA2019/exercise2/DA_AS02-06.csv")
df
df <- na.omit(df)
DT <- dist(df, method = "euclidean")
DT <- dist(df, method = "euclidean")
DTF <- hclust(DT, method = "ward.D")
plot(DTF)
groups <- cutree(DTF, k = 4)
rect.hclust(DTF, k = 4, border = "red")
library(psych)
describeBy(df, groups)
# 1. 데이터 불러오고 간단한 정제
df <- read.csv("C:/DA2019/exercise2/DA_AS02-06.csv")
df <- na.omit(df)
# K-평균 군집분석에서의 군집 수 결정
fit <- kmeans(df, 4)
# 3. 군집 시각화
library(cluster)
clusplot(df, fit$cluster, color = TRUE, labels = 2, lines = 0)
library(fpc)
plotcluster(UHC, fit$cluster)
plotcluster(df, fit$cluster)
##### DACH 13 분류분석 #####
# (교안)
loc<-"http://archive.ics.uci.edu/ml/machine-learning-databases/"
ds <-"breast-cancer-wisconsin/breast-cancer-wisconsin.data"
url<-paste(loc, ds, sep="")
url
breast <-read.table(url, sep=",", header=FALSE, na.strings= "?")
View(breast)
View(breast)
names(breast) <-c("ID", "ClumpTHickness","sizeUniformity",
"shapeUniformity","maginalAdhesion",
"SingleEpithelialCellsize","bareNuclei",
"BlandChromatin","normalNucleoli","mitosis","class")
View(breast)
View(breast)
df<-breast[-1]
df
View(df)
df$class<-factor(df$class, levels=c(2,4),
labels = c("benign", "malignant")) # class에 이름 붙이기
View(df)
set.seed(1234)
train <-sample(nrow(df), 0.7*nrow(df))
train
df.train<-df[train,]
View(df.train)
table(df.train$class)
table(df.validate$class)
df.validate<-df[-train,]
table(df.validate$class)
fit.logit<-glm(class~.,data=df.train,family= binomial())
summary(fit.logit)
prob<-predict(fit.logit, df.validate,type="response")
logit.pred<-factor(prob>.5, levels = c(FALSE, TRUE),
labels = c("benign", "malignant"))
logit.pref<-table(df.validate$class,logit.pred,
dnn= c("Actual","Predicted"))
logit.pref
logit.pred
logit.pref<-table(df.validate$class,logit.pred,
dnn= c("Actual","Predicted"))
logit.pref
# predict
fit.logit<-glm(class~.,data=df.train,family= binomial())
table(df.validate$class)
logit.fit.reduced<-step(fit.logit)
logit.fit.reduced<-step(fit.logit)
prob<-predict(logit.fit.reduced, df.validate,type="response")
logit.pred<-factor(prob>.5, levels = c(FALSE, TRUE),
labels = c("benign", "malignant"))
logit.pref<-table(df.validate$class,logit.pred,
dnn= c("Actual","Predicted"))
# Tunning
logit.fit.reduced<-step(fit.logit)
prob<-predict(logit.fit.reduced, df.validate,type="response")
logit.pred<-factor(prob>.5, levels = c(FALSE, TRUE),
labels = c("benign", "malignant"))
logit.pref<-table(df.validate$class,logit.pred,
dnn= c("Actual","Predicted"))
logit.pref
# <의사결정나무 기반 분류분석>
pkgs <-c("rpart", "rpart.plot", "party")
install.packages(pkgs, depend = TRUE)
library(rpart)
set.seed(1234)
dtree<-rpart(class~., data = df.train, method = "class", parms= list(split="information"))
dtree$cptable
plotcp(dtree)
dtree.pruned<-prune(dtree, cp= .0125)
library(rpart.plot)
prp(dtree.pruned, type = 2, extra = 104,
fallen.leaves= TRUE, main="Decision Tree")
dtree.pred<-predict(dtree.pruned, df.validate,type="class")
dtree.perf<-table(df.validate$class, dtree.pred, dnn= c("Actual", "Predicted"))
dtree.perf
logit.pref
# <조건적 추론 나무 기반 분류분석>
df<-breast[-1]
df$class<-factor(df$class, levels=c(2,4),
labels=c("benign", "malignant"))
set.seed(1234)
train<-sample(nrow(df), 0.7*nrow(df))
df.train<-df[train,]
df.validate<-df[-train,]
table(df.train$class)
table(df.validate$class)
library(party)
fit.ctree <-ctree(class~., data = df.train)
plot(fit.ctree, main = "Conditional Inference Trees")
ctree.pred <-predict(fit.ctree, df.validate, type = "response")
ctree.perf <-table(df.validate$class, ctree.pred,
dnn = c("Actual", "Predicted"))
ctree.perf
df <- read.csv("C:/DA2019/191107_CH06/DA예제-0601.csv")
df
fit <- aov(satis~com, data=df)
anova(fit)
boxplot(satis~com, col="sky blue", data = df)
TukeyHSD(fit)
df <-read.csv("C:/DA2019/191107_CH06//DA예제-0603.csv")
df
fit <- aov(yield ~ tem + pre + tem*pre, data = df)
anova(fit)
# < 다변량분산분석 >
df <-read.csv("C:/DA2019/191107_CH06/DA예제-0606.csv")
df
fit <-manova(cbind(y1,y2) ~ x1*x2, data = df)
summary(fit, test = "Pillai")
# (과제)
df <- read.csv("C:/DA2019/exercise2/DA_AS02-04.csv")
df
fit <- lm(y ~ x1 + x2 + x3, data = df)
library(car)
vif(fit)
plot(fit)
summary(fit)
# < 주성분분석 >
df <- read.csv("C:/DA2019/exercise2/DA_AS02-06.csv")
head(df)
mydata <-na.omit(df)
head(mydata)
# 1. 변수들 간 상관관계 분석
cor(mydata)
# 3. 요인 수 결정
fit <-princomp(mydata, cor = TRUE)
summary(fit) # 4개(누적분산 64.69%) - 6개(누적분산 77.30%) 가 적당.
e_value = eigen(cor(df))
e_value # 4개가 적정
# 4. 요인부하량(각 변수와 요인간의 상관관계) 산출
loadings(fit)
alpha(FA1, na.rm = TRUE)
