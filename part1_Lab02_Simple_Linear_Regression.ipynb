{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part1_Lab02_Simple_Linear_Regression",
      "provenance": [],
      "authorship_tag": "ABX9TyPj1Ew8+kFDXHxdHSU9ZChN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mong-Gu/DataScience/blob/master/part1_Lab02_Simple_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBa-4AOahVd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6plft3khYcX",
        "colab_type": "text"
      },
      "source": [
        "#**가설함수 H(x) = Wx + b**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNqc9YhatiN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [1, 2, 3, 4, 5]\n",
        "y_data = [1, 2, 3, 4, 5]\n",
        "\n",
        "W = tf.Variable(2.9) # 기울기\n",
        "b = tf.Variable(0.5) # 절편\n",
        "\n",
        "hypothesis = W * x_data + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_Z5JUwth3BO",
        "colab_type": "text"
      },
      "source": [
        "x_data 와 y_data가 위와 같은 상태라면, 최적의 W = 1, b = 0이다. 이후 W와 b가 조정될 것인지 확인해보자.\n",
        "#### <참고> 일반적으로 W와 b의 초기값은 랜덤값으로 구현한다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMSuFNpijXHe",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu15Y0a8khj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCZVK3JQhgcS",
        "colab_type": "text"
      },
      "source": [
        "# **비용함수 cost(W, b)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yfJvuAahUPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cost = tf.reduce_mean(tf.square(hypothesis - y_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XguhNvMEikE6",
        "colab_type": "text"
      },
      "source": [
        "오차의 제곱의 합을 평균, 즉 MSE(Mean Squared Error) 기반의 비용함수이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kX0KpHRjntH",
        "colab_type": "text"
      },
      "source": [
        "* tf_reduce_mean() : 차원(랭크)이 줄어들면서 평균을 구하는 함수\n",
        "\n",
        "  ex)<br>\n",
        "  v = [1., 2., 3., 4.]<br>\n",
        "  tf.reduce_mean(v) # 2.5<br>\n",
        "  이처럼 1차원에서 0차원으로 줄어들게 된다\n",
        "\n",
        "  <br>\n",
        "\n",
        "* tf.square() : parameter를 제곱해주는 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUVYBnhukn6T",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFIFMhXtkqwV",
        "colab_type": "text"
      },
      "source": [
        "# **경사하강법 (Gradient Descent)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgqUvvlYk1lu",
        "colab_type": "text"
      },
      "source": [
        "비용을 최소화하는 알고리즘은 많은데, 그중 가장 흔히 쓰이는 것이 경사하강법이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygceEnTmk1ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Learning_rate initialize\n",
        "learning_rate = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkkyUp-SkqTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0dd7a346-5fcf-4a23-f4d3-914247f88eb3"
      },
      "source": [
        "# Gradient Descent\n",
        "with tf.GradientTape() as tape:\n",
        "  hypothesis = W * x_data + b\n",
        "  cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "\n",
        "W.assign_sub(learning_rate * W_grad)\n",
        "b.assign_sub(learning_rate * b_grad)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=0.376>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK_4EzFZlsvm",
        "colab_type": "text"
      },
      "source": [
        "tensorflow에서는 Gradient Descent를 일반적으로 with구문과 함께 GradientTape()으로 구현한다.\n",
        "\n",
        "<순서>\n",
        "1. (line 2~4)\n",
        "with 구문 안에 있는 변수 정보를 tape에 기록한다. 위 예에서는 W와 b가 담길 것이다.\n",
        "2. (line 6)\n",
        "이후 tape에 gradient method를 호출하여 경사도값(기울기), 즉 미분값을 구한다.\n",
        "  - gradient method는 cost 함수에 대해서 변수들 (W, b)에 대한 개별 미분값을 구하여 튜플로 반환한다.\n",
        "  - 반환된 값들은 순서대로 W_grad과 b_grad에 담긴다. 즉, W에 대한 미분값은 W_grad에 순서대로 튜플로 저장되고, b에 대한 미분값은 b_grad에 순서대로 튜플로 저장된다.\n",
        "3. (line 8, 9)\n",
        "W와 b를 업데이트해준다.\n",
        "  - 여기서 사용된 A.assign_sub(B)는 python에서 'A = A - B' 혹은 ' A -= B'와 동일한 연산을 수행한다. \n",
        "    - 다만 tensorflow 내에서는 python 문법을 활용할 수 없기 때문에 assign_sub를 활용한다.\n",
        "  - 각 gradient값에 learning_rate 값을 곱해서 업데이트해주고 있는데, learning_rate는 gradient값을 얼마만큼 반영해줄 것인지 정해준다.\n",
        "    - 일반적으로 learning_rate는 굉장히 작은 값으로 할당한다. 0.01, 0.001, 0.0001, .... learning_rate를 너무 많이 할당해버리면 빠르게 학습을 할 수 있겠지만, 정확도가 떨어질 위험이 있다. 조금씩 차근차근 반영해나가며 최적의 W와 b를 찾아나가는 것이 더 좋다.\n",
        "\n",
        "\n",
        "**위 과정이 W와 b를 업데이트하는 한 걸음이라고 생각하자.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkADsKWjo-oP",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRVa_YcDpC6q",
        "colab_type": "text"
      },
      "source": [
        "# **위 과정을 모두 반영하여 총정리한 Full Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s4kiQChlkQG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "be26af56-2697-4605-9ab0-43f867a25b20"
      },
      "source": [
        "import tensorflow as tf\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "# Data\n",
        "x_data = [1, 2, 3, 4, 5]\n",
        "y_data = [1, 2, 3, 4, 5]\n",
        "\n",
        "# W, b initialize\n",
        "W = tf.Variable(2.9)\n",
        "b = tf.Variable(0.5)\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Update both W and b\n",
        "for i in range(100+1):\n",
        "  # Gradient Descent\n",
        "  with tf.GradientTape() as tape:\n",
        "    hypothesis = W * x_data + b\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "  \n",
        "  W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "  W.assign_sub(learning_rate * W_grad)\n",
        "  b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    print(\"{:5}|{:10.4f}|{:10.4}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0|    2.4520|     0.376| 45.660004\n",
            "   10|    1.1036|  0.003398|  0.206336\n",
            "   20|    1.0128|  -0.02091|  0.001026\n",
            "   30|    1.0065|  -0.02184|  0.000093\n",
            "   40|    1.0059|  -0.02123|  0.000083\n",
            "   50|    1.0057|  -0.02053|  0.000077\n",
            "   60|    1.0055|  -0.01984|  0.000072\n",
            "   70|    1.0053|  -0.01918|  0.000067\n",
            "   80|    1.0051|  -0.01854|  0.000063\n",
            "   90|    1.0050|  -0.01793|  0.000059\n",
            "  100|    1.0048|  -0.01733|  0.000055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNOfWYJ8qmzO",
        "colab_type": "text"
      },
      "source": [
        "i = 0 (반복 횟수; Epoch = 1)일 때는 W가 2.4520, b가 0.3760인 반면, 학습이 이루어질수록 앞서 생각했던 W = 1, b = 0으로 수렴하고 있다. 즉, 점점 cost가 줄어들고 있다는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u8bSCnLrUIw",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLUfp9AXrWzs",
        "colab_type": "text"
      },
      "source": [
        "그렇다면 만들어놓은 가설 함수는 최종적으로 다음과 같은 모양을 가지게 된다.<br>\n",
        "**H(x) = 1.0048 * x - 0.01733**\n",
        "\n",
        "이 가설 함수로 새로운 값을 예측해보자,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nONt_65upN_j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b27e99fa-8522-4677-ac74-fc551a8dea8c"
      },
      "source": [
        "print(W * 5 + b)\n",
        "# 기존의 x_data에서는 x = 5일 때 y = 5였다. \n",
        "# y가 5에 가까울수록 정확한 가설 함수를 세웠다고 할 수 있다."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(5.00667, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxLSZk2srSq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "496caf29-a744-4f5a-d1cc-b418b42d659d"
      },
      "source": [
        "print(W * 2.5 + b)\n",
        "# 기존의 x_data와 y_data를 살펴봤을 때, \n",
        "# y는 2.5에 가까울수록 정확한 가설 함수를 세웠다고 할 수 있다."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.4946702, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}